{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duongtran96/Project_LLM/blob/main/RAG_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WfoxgDXXcxD"
      },
      "source": [
        "# **Trắc Nghiệm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmwf9twfXaGa"
      },
      "outputs": [],
      "source": [
        "#1: B Retrieval Augmented Generation.\n",
        "#2: A Truy vấn thêm các thông tin bên ngoài.\n",
        "#3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the necessary library packages\n"
      ],
      "metadata": {
        "id": "577f3Q9O0Y5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNsJWDaBfvEI"
      },
      "outputs": [],
      "source": [
        "!pip install  -q transformers==4.41.2\n",
        "!pip install  -q  bitsandbytes==0.43.1\n",
        "!pip install  -q  accelerate==0.31.0\n",
        "!pip install  -q  langchain==0.2.5\n",
        "!pip install  -q  langchainhub==0.1.20\n",
        "!pip install  -q  langchain-chroma==0.1.1\n",
        "!pip install  -q  langchain-community==0.2.5\n",
        "!pip install  -q  langchain-openai==0.1.9\n",
        "!pip install  -q  langchain_huggingface==0.0.3\n",
        "!pip install  -q  chainlit==1.1.304\n",
        "!pip install  -q  python-dotenv==1.0.1\n",
        "!pip install  -q  pypdf==4.2.0\n",
        "!pip install  -q  numpy=1.24.2\n",
        "!npm install  -g  localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup vector database"
      ],
      "metadata": {
        "id": "JvAFSzo_0g3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jPSnSuwg5tB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "## Read File pdf\n",
        "Loader = PyPDFLoader\n",
        "FILE_PATH = \"/content/YOLOv10_Tutorials.pdf\"\n",
        "loader = Loader(FILE_PATH)\n",
        "documents = loader.load()\n",
        "\n",
        "## Text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"Number of sub-documents: \", len(docs))\n",
        "print(docs[0])\n",
        "\n",
        "##Initialize instance vectorization\n",
        "embedding = HuggingFaceEmbeddings()\n",
        "\n",
        "## Initialize Vector Database\n",
        "vector_db = Chroma.from_documents(documents = docs, embedding = embedding)\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "## Initialize Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JDLt1blwOe"
      },
      "source": [
        "# **Initialize** Large Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp1DS_lSl0_V"
      },
      "outputs": [],
      "source": [
        "## Declare some settings needed for the model\n",
        "nf4_config =  BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = 'nf4',\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16\n",
        ")\n",
        "\n",
        "## Initizalize model and tokenizer\n",
        "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config = nf4_config,\n",
        "    low_cpu_mem_usage = True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "## Integrate tokenizer and model into one pipeline for ease of use\n",
        "model_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    max_new_tokens = 512,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "    device_map = 'auto'\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = model_pipeline,\n",
        ")\n",
        "\n",
        "## run program\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USER_QUESTION = \"Bài toán Object Detection\"\n",
        "output = rag_chain.invoke(USER_QUESTION)\n",
        "answer = output.split(\"Answer: \")[1].strip()\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "INib_OVXNuSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwtOT/0261fAwbnfXWLUuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}